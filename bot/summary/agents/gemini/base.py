import os
from datetime import datetime
from pydantic import BaseModel
import google.generativeai as genai
from google.generativeai import protos
from google.ai.generativelanguage_v1beta.types import content
from typing import Optional
from textwrap import dedent
import logging

from .exceptions import GeminiUnexpectedFinishReason, GeminiModelError
from bot.settings import settings

import proto


# From google.ai.generativelanguage_v1beta.types.Candidate
# google/ai/generativelanguage_v1beta/types/generative_service.py
class FinishReason(proto.Enum):
    r"""Defines the reason why the model stopped generating tokens.

    Values:
        FINISH_REASON_UNSPECIFIED (0):
            Default value. This value is unused.
        STOP (1):
            Natural stop point of the model or provided
            stop sequence.
        MAX_TOKENS (2):
            The maximum number of tokens as specified in
            the request was reached.
        SAFETY (3):
            The response candidate content was flagged
            for safety reasons.
        RECITATION (4):
            The response candidate content was flagged
            for recitation reasons.
        LANGUAGE (6):
            The response candidate content was flagged
            for using an unsupported language.
        OTHER (5):
            Unknown reason.
        BLOCKLIST (7):
            Token generation stopped because the content
            contains forbidden terms.
        PROHIBITED_CONTENT (8):
            Token generation stopped for potentially
            containing prohibited content.
        SPII (9):
            Token generation stopped because the content
            potentially contains Sensitive Personally
            Identifiable Information (SPII).
        MALFORMED_FUNCTION_CALL (10):
            The function call generated by the model is
            invalid.
    """
    FINISH_REASON_UNSPECIFIED = 0
    STOP = 1
    MAX_TOKENS = 2
    SAFETY = 3
    RECITATION = 4
    LANGUAGE = 6
    OTHER = 5
    BLOCKLIST = 7
    PROHIBITED_CONTENT = 8
    SPII = 9
    MALFORMED_FUNCTION_CALL = 10

class ChatModelConfig(BaseModel):
    """
    Configuration for a Gemini chat model.

    Attributes:
        session_id (str): ID of the session, used to identify the session of several agents
        agent_id (str): ID of the agent, used to identify the agent in the session
        llm_model_name (str): Name of the Gemini model to use
        temperature (float): Controls randomness in the model's responses. 
            Higher values (e.g., 0.8) make output more random, lower values (e.g., 0.2) make it more focused
        system_prompt (str): Initial system prompt to set model behavior and context
        response_schema (Optional[content.Schema]): Schema defining the expected response format.
            If provided, responses will be formatted as JSON matching this schema.
        max_tokens (Optional[int]): Maximum number of tokens in the response.
    """
    session_id: str = ""
    agent_id: str = ""
    llm_model_name: str = "gemini-1.5-flash-002"
    temperature: float = 1.0
    system_prompt: str = ""
    response_schema: Optional[content.Schema] = None
    max_tokens: Optional[int] = 500

    class Config:
        arbitrary_types_allowed = True

class BaseChatModel:
    """Base class for interacting with Google's Gemini chat models.
    
    This class provides core functionality for configuring and interacting with
    Gemini models, including:
    - Model initialization with customizable configuration (temperature, tokens, etc.)
    - Response schema validation and JSON formatting
    - Chat history tracking
    - Response logging and debugging capabilities
    - System prompt configuration
    
    The class is designed to be extended by specialized chat agents that implement
    specific use cases like summarization, translation, etc.
    """

    def __init__(self, config: ChatModelConfig):
        """Initialize a new BaseChatModel instance.

        Args:
            config (ChatModelConfig): Configuration for the chat model
        """
        generation_config = genai.types.GenerationConfig(
            temperature=config.temperature,
            top_p=0.95,
            top_k=40,
            max_output_tokens=config.max_tokens,
        )
        
        if config.response_schema:
            generation_config.response_schema = config.response_schema
            generation_config.response_mime_type = "application/json"
            
        self.model = genai.GenerativeModel(
            model_name=config.llm_model_name,
            system_instruction=config.system_prompt,
            generation_config=generation_config
        )

        self._history: list[protos.Content] = []

        self._session_id = config.session_id
        self._agent_id = config.agent_id

    def _save_response(self, response: dict):
        """Save the raw response from the Gemini model to a file for debugging/logging purposes.
        
        If enabled via settings, saves the complete model response to a timestamped file
        in a directory structure organized by session ID. The agent_id is included in the 
        filename to distinguish responses from different agents within the same session.

        Args:
            response (dict): The raw response dictionary from the Gemini model
        """
        
        if settings.keep_raw_engine_responses:
            response_dir = os.path.join(settings.raw_engine_responses_dir, self._session_id)
            os.makedirs(response_dir, exist_ok=True)
            file_path = os.path.join(
                response_dir,
                f"{self._agent_id}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.txt"
            )
            with open(file_path, "w") as f:
                f.write(str(response))
    
    def _generate_response(self, prompt: str) -> str:
        """Generate a response from the model based on the given prompt.

        The prompt is added to the conversation history before generating the response.
        The response is also added to the history for context in future interactions,
        but only if generation is successful (finishes with STOP reason).
        If there's an error or unexpected finish reason, the prompt is removed from history.

        Args:
            prompt (str): The input text to send to the model

        Returns:
            str: The generated response text from the model

        Raises:
            GeminiModelError: If there is an error generating the response
            GeminiUnexpectedFinishReason: If the model stops for an unexpected reason
        """

        logger = logging.getLogger(self.__class__.__module__)

        # Add prompt to history
        prompt_content = protos.Content(parts=[protos.Part(text=dedent(prompt))], role="user")
        self._history.append(prompt_content)
        
        try:    
            response = self.model.generate_content(self._history)
        except Exception as e:
            # Roll back the prompt from history on error
            self._history.pop()
            logger.error(f"Error generating response: {e}")
            raise GeminiModelError(f"Error generating response: {e}") from e

        self._save_response(response.candidates[0])

        finish_reason = FinishReason(response.candidates[0].finish_reason)

        if finish_reason == FinishReason.STOP:
            self._history.append(response.candidates[0].content)
            return response.candidates[0].content.parts[0].text
        else:
            # Roll back the prompt from history on error
            self._history.pop()
            logger.error(f"Unexpected finish reason: {finish_reason.name}")
            raise GeminiUnexpectedFinishReason(f"{finish_reason.name}")
